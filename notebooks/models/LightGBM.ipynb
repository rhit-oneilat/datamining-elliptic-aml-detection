{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import networkx as nx\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import dask.bag as db\n",
                "import dask.distributed as Client\n",
                "import lightgbm as lgb\n",
                "import warnings\n",
                "import time\n",
                "import multiprocessing\n",
                "from joblib import Parallel, delayed\n",
                "from sklearn.model_selection import train_test_split, KFold\n",
                "from sklearn.base import BaseEstimator, TransformerMixin\n",
                "from sklearn.metrics import roc_auc_score\n",
                "from imblearn.over_sampling import ADASYN\n",
                "from imblearn.under_sampling import TomekLinks\n",
                "from imblearn.pipeline import Pipeline as ImbPipeline\n",
                "from typing import Dict, List, Tuple\n",
                "from contextlib import contextmanager\n",
                "from functools import partial\n",
                "import time"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "class EnhancedSampler(BaseEstimator, TransformerMixin):\n",
                "    def __init__(self, sampling_strategy='auto'):\n",
                "        self.sampling_strategy = sampling_strategy\n",
                "        self.pipeline = ImbPipeline([\n",
                "            ('adasyn', ADASYN(sampling_strategy=sampling_strategy)),\n",
                "            ('tomek', TomekLinks())\n",
                "        ])\n",
                "    \n",
                "    def fit(self, X, y):\n",
                "        return self\n",
                "    \n",
                "    def transform(self, X, y=None):\n",
                "        if y is not None:\n",
                "            X_resampled, y_resampled = self.pipeline.fit_resample(X, y)\n",
                "            return X_resampled, y_resampled\n",
                "        return X"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load and preprocess data\n",
                "edge_df = pd.read_csv(\"../../data/elliptic_txs_edgelist.csv\")\n",
                "class_df = pd.read_csv(\"../../data/elliptic_txs_classes.csv\")\n",
                "\n",
                "edge_df.rename(columns={\"txId1\": \"source\", \"txId2\": \"target\"}, inplace=True)\n",
                "merged_df = edge_df.merge(class_df, left_on=\"source\", right_on=\"txId\", how=\"left\")\n",
                "merged_df = merged_df.merge(\n",
                "    class_df,\n",
                "    left_on=\"target\",\n",
                "    right_on=\"txId\",\n",
                "    how=\"left\",\n",
                "    suffixes=(\"_source\", \"_target\"),\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create Directed Graph\n",
                "def create_directed_graph(edge_df):\n",
                "    G_dir = nx.DiGraph()\n",
                "    G_dir.add_nodes_from(class_df[\"txId\"])\n",
                "    G_dir.add_edges_from(edge_df[[\"source\", \"target\"]].values)\n",
                "    return G_dir\n",
                "\n",
                "G_dir = create_directed_graph(edge_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_enhanced_features_batch(nodes, G, max_hops=3):\n",
                "    \"\"\"Compute features for a batch of nodes to reduce graph copying overhead\"\"\"\n",
                "    features_dict = {}\n",
                "    for node in nodes:\n",
                "        subgraph = nx.ego_graph(G, node, radius=max_hops, undirected=False)\n",
                "        \n",
                "        features = {\n",
                "            \"num_nodes\": subgraph.number_of_nodes(),\n",
                "            \"num_edges\": subgraph.number_of_edges(),\n",
                "            \"in_degree\": subgraph.in_degree(node),\n",
                "            \"out_degree\": subgraph.out_degree(node),\n",
                "            \"degree_centrality\": nx.degree_centrality(subgraph)[node],\n",
                "            \"in_degree_centrality\": nx.in_degree_centrality(subgraph)[node],\n",
                "            \"out_degree_centrality\": nx.out_degree_centrality(subgraph)[node],\n",
                "            \"pagerank\": nx.pagerank(subgraph)[node],\n",
                "            \"clustering_coeff\": nx.clustering(subgraph.to_undirected(), node),\n",
                "            \"local_clustering_coeff\": nx.average_clustering(subgraph.to_undirected()),\n",
                "        }\n",
                "        \n",
                "        # More expensive computations\n",
                "        try:\n",
                "            features[\"average_neighbor_degree\"] = np.mean(\n",
                "                [subgraph.degree(n) for n in subgraph.neighbors(node)]\n",
                "            ) if list(subgraph.neighbors(node)) else 0\n",
                "        except:\n",
                "            features[\"average_neighbor_degree\"] = 0\n",
                "            \n",
                "        try:\n",
                "            features[\"connectivity_ratio\"] = (subgraph.number_of_edges() / \n",
                "                (subgraph.number_of_nodes() * (subgraph.number_of_nodes() - 1) + 1))\n",
                "        except:\n",
                "            features[\"connectivity_ratio\"] = 0\n",
                "            \n",
                "        try:\n",
                "            features[\"strongly_connected_components\"] = len(list(nx.strongly_connected_components(subgraph)))\n",
                "        except:\n",
                "            features[\"strongly_connected_components\"] = 0\n",
                "            \n",
                "        try:\n",
                "            features[\"weakly_connected_components\"] = len(list(nx.weakly_connected_components(subgraph)))\n",
                "        except:\n",
                "            features[\"weakly_connected_components\"] = 0\n",
                "            \n",
                "        try:\n",
                "            features[\"harmonic_centrality\"] = nx.harmonic_centrality(subgraph)[node]\n",
                "        except:\n",
                "            features[\"harmonic_centrality\"] = 0\n",
                "            \n",
                "        try:\n",
                "            features[\"eigenvector_centrality\"] = nx.eigenvector_centrality(\n",
                "                subgraph,\n",
                "                max_iter=100,\n",
                "                tol=1e-2\n",
                "            ).get(node, 0)\n",
                "        except:\n",
                "            features[\"eigenvector_centrality\"] = 0\n",
                "            \n",
                "        try:\n",
                "            strongly_components = list(nx.strongly_connected_components(subgraph))\n",
                "            features[\"max_strongly_connected_components\"] = max(len(c) for c in strongly_components) if strongly_components else 0\n",
                "        except:\n",
                "            features[\"max_strongly_connected_components\"] = 0\n",
                "            \n",
                "        try:\n",
                "            features[\"betweenness_centrality\"] = nx.betweenness_centrality(subgraph)[node]\n",
                "        except:\n",
                "            features[\"betweenness_centrality\"] = 0\n",
                "            \n",
                "        features_dict[node] = features\n",
                "        \n",
                "    return features_dict"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_features_parallel(G, merged_df, node_class, max_hops=3, n_jobs=-1):\n",
                "    \"\"\"Extract features with optimized batching for many CPUs\"\"\"\n",
                "    start_time = time.time()\n",
                "    \n",
                "    # Get nodes for the specified class\n",
                "    nodes = merged_df[merged_df[\"class_source\"] == str(node_class)][\"source\"].tolist()\n",
                "    total_nodes = len(nodes)\n",
                "    \n",
                "    # Calculate optimal batch size for 256 CPUs\n",
                "    n_cpus = n_jobs if n_jobs > 0 else multiprocessing.cpu_count()\n",
                "    \n",
                "    # Aim for 2 batches per CPU to keep all cores busy\n",
                "    target_n_batches = n_cpus * 2\n",
                "    batch_size = max(1, total_nodes // target_n_batches)\n",
                "    \n",
                "    # Split nodes into batches\n",
                "    node_batches = [nodes[i:i + batch_size] for i in range(0, len(nodes), batch_size)]\n",
                "    n_batches = len(node_batches)\n",
                "    \n",
                "    print(f\"Processing {total_nodes} nodes in {n_batches} batches\")\n",
                "    print(f\"Batch size: {batch_size} nodes\")\n",
                "    print(f\"Using {n_cpus} CPU cores\")\n",
                "    \n",
                "    # Process batches in parallel using multiprocessing\n",
                "    results = Parallel(n_jobs=n_jobs, verbose=1, prefer=\"processes\")(\n",
                "        delayed(compute_enhanced_features_batch)(batch, G, max_hops)\n",
                "        for batch in node_batches\n",
                "    )\n",
                "    \n",
                "    # Merge results\n",
                "    combined_features = {}\n",
                "    for result in results:\n",
                "        combined_features.update(result)\n",
                "    \n",
                "    end_time = time.time()\n",
                "    elapsed = end_time - start_time\n",
                "    nodes_per_second = total_nodes / elapsed\n",
                "    print(f\"Processed {total_nodes} nodes in {elapsed:.2f} seconds\")\n",
                "    print(f\"Processing speed: {nodes_per_second:.1f} nodes/second\")\n",
                "    \n",
                "    return combined_features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract features for each class with batching\n",
                "class_1_features = extract_features_parallel(\n",
                "    G_dir, \n",
                "    merged_df, \n",
                "    node_class=1, \n",
                "    n_jobs=256 # Adjust based on your graph size\n",
                ")\n",
                "class_2_features = extract_features_parallel(\n",
                "    G_dir, \n",
                "    merged_df, \n",
                "    node_class=2, \n",
                "    n_jobs=256\n",
                ")\n",
                "\n",
                "all_features = pd.DataFrame.from_dict(\n",
                "    {**class_1_features, **class_2_features}, orient=\"index\"\n",
                ")\n",
                "all_features[\"class\"] = [\"1\"] * len(class_1_features) + [\"2\"] * len(class_2_features)\n",
                "\n",
                "# Prepare the data\n",
                "X = all_features.drop(columns=[\"class\"])\n",
                "y = all_features[\"class\"].astype(int)\n",
                "\n",
                "# Split the data\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.3, random_state=42, stratify=y\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "@contextmanager\n",
                "def timer(name: str):\n",
                "    \"\"\"Context manager for timing code blocks\"\"\"\n",
                "    start = time.time()\n",
                "    yield\n",
                "    print(f\"{name} took {time.time() - start:.2f} seconds\")\n",
                "\n",
                "def calculate_scale_pos_weight(y: np.ndarray) -> float:\n",
                "    \"\"\"Calculate scale_pos_weight for imbalanced datasets\"\"\"\n",
                "    return np.sum(y == 0) / np.sum(y == 1)\n",
                "\n",
                "def get_default_params(scale_pos_weight: float, num_cpu_per_model: int) -> Dict:\n",
                "    \"\"\"Get default LightGBM parameters optimized for HPC\"\"\"\n",
                "    return {\n",
                "        'objective': 'binary',\n",
                "        'metric': 'auc',\n",
                "        'boosting_type': 'gbdt',\n",
                "        'num_leaves': 255,  # Increased for more powerful machine\n",
                "        'learning_rate': 0.05,\n",
                "        'feature_fraction': 0.9,\n",
                "        'bagging_fraction': 0.8,\n",
                "        'bagging_freq': 5,\n",
                "        'verbose': -1,\n",
                "        'num_threads': num_cpu_per_model,  # Dedicated CPUs per model\n",
                "        'scale_pos_weight': scale_pos_weight,\n",
                "        'deterministic': True,\n",
                "        'force_col_wise': True,\n",
                "        'min_data_in_leaf': 20,\n",
                "        'max_bin': 255,\n",
                "        'device_type': 'cpu',  # Explicitly set device type\n",
                "        'linear_tree': False,  # Better for parallel processing\n",
                "        'histogram_pool_size': -1,  # Use all available memory for histograms\n",
                "        'use_missing': True,  # Optimize handling of missing values\n",
                "        'feature_pre_filter': False,  # Disable pre-filtering for better parallel performance\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_fold(fold: int, \n",
                "               train_idx: np.ndarray, \n",
                "               val_idx: np.ndarray, \n",
                "               X_train: pd.DataFrame, \n",
                "               y_train: pd.Series, \n",
                "               X_test: pd.DataFrame, \n",
                "               params: Dict, \n",
                "               feature_names: List[str]) -> Tuple:\n",
                "    \"\"\"Train a single fold with dedicated resources\"\"\"\n",
                "    # Split data\n",
                "    X_train_fold = X_train.iloc[train_idx]\n",
                "    y_train_fold = y_train.iloc[train_idx]\n",
                "    X_val_fold = X_train.iloc[val_idx]\n",
                "    y_val_fold = y_train.iloc[val_idx]\n",
                "    \n",
                "    # Create datasets\n",
                "    train_set = lgb.Dataset(\n",
                "        X_train_fold, \n",
                "        y_train_fold, \n",
                "        feature_name=feature_names,\n",
                "        free_raw_data=True\n",
                "    )\n",
                "    val_set = lgb.Dataset(\n",
                "        X_val_fold, \n",
                "        y_val_fold, \n",
                "        feature_name=feature_names,\n",
                "        free_raw_data=True,\n",
                "        reference=train_set\n",
                "    )\n",
                "    \n",
                "    # Train model\n",
                "    with warnings.catch_warnings():\n",
                "        warnings.simplefilter(\"ignore\")\n",
                "        model = lgb.train(\n",
                "            params,\n",
                "            train_set,\n",
                "            num_boost_round=1000,\n",
                "            valid_sets=[val_set],\n",
                "            callbacks=[\n",
                "                lgb.early_stopping(stopping_rounds=50),\n",
                "                lgb.log_evaluation(period=100)\n",
                "            ]\n",
                "        )\n",
                "    \n",
                "    # Evaluate\n",
                "    val_preds = model.predict(X_val_fold)\n",
                "    cv_score = roc_auc_score(y_val_fold, val_preds)\n",
                "    \n",
                "    # Get feature importance\n",
                "    importance = pd.DataFrame({\n",
                "        'feature': feature_names,\n",
                "        'importance': model.feature_importance('gain'),\n",
                "        'fold': fold\n",
                "    })\n",
                "    \n",
                "    # Make test predictions if test data is provided\n",
                "    test_predictions = model.predict(X_test) if X_test is not None else None\n",
                "    \n",
                "    return model, cv_score, importance, test_predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LightGBMCV:\n",
                "    def __init__(self, \n",
                "                 params: Dict = None, \n",
                "                 n_splits: int = 5, \n",
                "                 random_state: int = 42, \n",
                "                 n_jobs: int = None):\n",
                "        self.params = params\n",
                "        self.n_splits = n_splits\n",
                "        self.random_state = random_state\n",
                "        self.n_jobs = n_jobs or min(multiprocessing.cpu_count(), 256)  # Default to available CPUs\n",
                "        self.models = []\n",
                "        self.feature_importance = None\n",
                "        self.cv_scores = []\n",
                "        \n",
                "    def train_and_evaluate(\n",
                "        self, \n",
                "        X_train: pd.DataFrame, \n",
                "        y_train: pd.Series, \n",
                "        X_test: pd.DataFrame = None\n",
                "    ) -> Tuple[List[float], pd.DataFrame, np.ndarray]:\n",
                "        \"\"\"\n",
                "        Train models with cross-validation using parallel processing\n",
                "        \"\"\"\n",
                "        with timer(\"Cross-validation training\"):\n",
                "            # Calculate CPUs per model\n",
                "            cpus_per_model = max(1, self.n_jobs // self.n_splits)\n",
                "            \n",
                "            # Calculate scale_pos_weight if not provided\n",
                "            if self.params is None:\n",
                "                scale_pos_weight = calculate_scale_pos_weight(y_train)\n",
                "                self.params = get_default_params(scale_pos_weight, cpus_per_model)\n",
                "            else:\n",
                "                self.params['num_threads'] = cpus_per_model\n",
                "            \n",
                "            # Setup cross-validation\n",
                "            kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n",
                "            \n",
                "            # Prepare feature names\n",
                "            feature_names = X_train.columns.tolist()\n",
                "            \n",
                "            # Train models in parallel\n",
                "            with Parallel(n_jobs=self.n_splits, backend='multiprocessing', verbose=1) as parallel:\n",
                "                results = parallel(\n",
                "                    delayed(train_fold)(\n",
                "                        fold, train_idx, val_idx, X_train, y_train, \n",
                "                        X_test, self.params, feature_names\n",
                "                    )\n",
                "                    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train))\n",
                "                )\n",
                "            \n",
                "            # Unpack results\n",
                "            self.models, self.cv_scores, importances, test_preds = zip(*results)\n",
                "            \n",
                "            # Aggregate feature importance\n",
                "            self.feature_importance = pd.concat(importances)\n",
                "            self.feature_importance = (\n",
                "                self.feature_importance.groupby('feature')\n",
                "                .agg({\n",
                "                    'importance': ['mean', 'std']\n",
                "                })\n",
                "                .reset_index()\n",
                "            )\n",
                "            self.feature_importance.columns = ['feature', 'importance_mean', 'importance_std']\n",
                "            self.feature_importance = self.feature_importance.sort_values(\n",
                "                'importance_mean', \n",
                "                ascending=False\n",
                "            )\n",
                "            \n",
                "            # Aggregate test predictions\n",
                "            test_predictions = (\n",
                "                np.mean(test_preds, axis=0) if X_test is not None else None\n",
                "            )\n",
                "            \n",
                "        return self.cv_scores, self.feature_importance, test_predictions\n",
                "\n",
                "    def get_feature_importance(self, top_n: int = None) -> pd.DataFrame:\n",
                "        \"\"\"Get feature importance with optional top N filter\"\"\"\n",
                "        if self.feature_importance is None:\n",
                "            raise ValueError(\"Model hasn't been trained yet\")\n",
                "        if top_n:\n",
                "            return self.feature_importance.head(top_n)\n",
                "        return self.feature_importance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize with high CPU count\n",
                "cv_model = LightGBMCV(n_splits=5, n_jobs=-1)  # Use all 256 CPUs\n",
                "\n",
                "# Train and evaluate\n",
                "cv_scores, feature_importance, test_predictions = cv_model.train_and_evaluate(\n",
                "    X_train, \n",
                "    y_train, \n",
                "    X_test\n",
                ")\n",
                "\n",
                "# Print results\n",
                "print(f\"CV Scores: {cv_scores}\")\n",
                "print(f\"Mean CV Score: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n",
                "\n",
                "# Get top 10 features\n",
                "top_features = cv_model.get_feature_importance(top_n=10)\n",
                "print(\"\\nTop 10 Features:\")\n",
                "print(top_features)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
