{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Elliptic Transactions Classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import networkx as nx\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import balanced_accuracy_score, make_scorer\n",
                "from imblearn.combine import SMOTEENN\n",
                "from sklearn.base import BaseEstimator, TransformerMixin\n",
                "from sklearn.pipeline import Pipeline\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### SMOTE transformer to use in pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Custom SMOTEENN transformer\n",
                "class SMOTEENNTransformer(BaseEstimator, TransformerMixin):\n",
                "    def __init__(self, sampling_strategy='auto'):\n",
                "        self.sampling_strategy = sampling_strategy\n",
                "        self.smoteenn = SMOTEENN(sampling_strategy=sampling_strategy)\n",
                "    \n",
                "    def fit(self, X, y):\n",
                "        return self\n",
                "    \n",
                "    def transform(self, X, y=None):\n",
                "        if y is not None:\n",
                "            X_resampled, y_resampled = self.smoteenn.fit_resample(X, y)\n",
                "            return X_resampled\n",
                "        return X"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### More enhanced sampler"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from imblearn.over_sampling import ADASYN\n",
                "from imblearn.under_sampling import TomekLinks\n",
                "from imblearn.pipeline import Pipeline as ImbPipeline\n",
                "\n",
                "class EnhancedSampler(BaseEstimator, TransformerMixin):\n",
                "    def __init__(self, sampling_strategy='auto'):\n",
                "        self.sampling_strategy = sampling_strategy\n",
                "        self.pipeline = ImbPipeline([\n",
                "            ('adasyn', ADASYN(sampling_strategy=sampling_strategy)),\n",
                "            ('tomek', TomekLinks())\n",
                "        ])\n",
                "    \n",
                "    def fit(self, X, y):\n",
                "        return self\n",
                "    \n",
                "    def transform(self, X, y=None):\n",
                "        if y is not None:\n",
                "            X_resampled, y_resampled = self.pipeline.fit_resample(X, y)\n",
                "            return X_resampled, y_resampled\n",
                "        return X"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Data Load"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load and preprocess data\n",
                "edge_df = pd.read_csv(\"elliptic_txs_edgelist.csv\")\n",
                "class_df = pd.read_csv(\"elliptic_txs_classes.csv\")\n",
                "\n",
                "edge_df.rename(columns={\"txId1\": \"source\", \"txId2\": \"target\"}, inplace=True)\n",
                "merged_df = edge_df.merge(class_df, left_on=\"source\", right_on=\"txId\", how=\"left\")\n",
                "merged_df = merged_df.merge(\n",
                "    class_df,\n",
                "    left_on=\"target\",\n",
                "    right_on=\"txId\",\n",
                "    how=\"left\",\n",
                "    suffixes=(\"_source\", \"_target\"),\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Create Digraph"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create Directed Graph\n",
                "def create_directed_graph(edge_df):\n",
                "    G_dir = nx.DiGraph()\n",
                "    G_dir.add_nodes_from(class_df[\"txId\"])\n",
                "    G_dir.add_edges_from(edge_df[[\"source\", \"target\"]].values)\n",
                "    return G_dir\n",
                "\n",
                "G_dir = create_directed_graph(edge_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Feature Compute"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_enhanced_features(G, node, max_hops=3):\n",
                "    subgraph = nx.ego_graph(G, node, radius=max_hops, undirected=False)\n",
                "    \n",
                "    features = {\n",
                "        \"num_nodes\": subgraph.number_of_nodes(),\n",
                "        \"num_edges\": subgraph.number_of_edges(),\n",
                "        \"in_degree\": subgraph.in_degree(node),\n",
                "        \"out_degree\": subgraph.out_degree(node),\n",
                "        \"degree_centrality\": nx.degree_centrality(subgraph)[node],\n",
                "        \"in_degree_centrality\": nx.in_degree_centrality(subgraph)[node],\n",
                "        \"out_degree_centrality\": nx.out_degree_centrality(subgraph)[node],\n",
                "        \"pagerank\": nx.pagerank(subgraph)[node],\n",
                "        \"clustering_coeff\": nx.clustering(subgraph.to_undirected(), node),\n",
                "        \"local_clustering_coeff\": nx.average_clustering(subgraph.to_undirected()),\n",
                "        \"average_neighbor_degree\": np.mean(\n",
                "            [subgraph.degree(n) for n in subgraph.neighbors(node)]\n",
                "        ) if list(subgraph.neighbors(node)) else 0,\n",
                "        \"connectivity_ratio\": subgraph.number_of_edges() / (subgraph.number_of_nodes() * (subgraph.number_of_nodes() - 1) + 1),\n",
                "        \"strongly_connected_components\": len(list(nx.strongly_connected_components(subgraph))),\n",
                "        \"weakly_connected_components\": len(list(nx.weakly_connected_components(subgraph))),\n",
                "    }\n",
                "\n",
                "    # Safe centrality calculations\n",
                "    try:\n",
                "        features[\"harmonic_centrality\"] = nx.harmonic_centrality(subgraph)[node]\n",
                "    except:\n",
                "        features[\"harmonic_centrality\"] = 0\n",
                "\n",
                "    try:\n",
                "        features[\"eigenvector_centrality\"] = nx.eigenvector_centrality(\n",
                "            subgraph, \n",
                "            max_iter=100,  # Reduced iterations\n",
                "            tol=1e-2       # Increased tolerance\n",
                "        ).get(node, 0)\n",
                "    except nx.PowerIterationFailedConvergence:\n",
                "        features[\"eigenvector_centrality\"] = 0\n",
                "\n",
                "    try:\n",
                "        strongly_components = list(nx.strongly_connected_components(subgraph))\n",
                "        features[\"max_strongly_connected_components\"] = max(len(c) for c in strongly_components) if strongly_components else 0\n",
                "    except:\n",
                "        features[\"max_strongly_connected_components\"] = 0\n",
                "\n",
                "    try:\n",
                "        features[\"betweenness_centrality\"] = nx.betweenness_centrality(subgraph)[node]\n",
                "    except:\n",
                "        features[\"betweenness_centrality\"] = 0\n",
                "\n",
                "    return features\n",
                "\n",
                "def extract_features_for_classes(G, merged_df, node_class, max_hops=3):\n",
                "    nodes = merged_df[merged_df[\"class_source\"] == str(node_class)][\"source\"].tolist()\n",
                "    features = {node: compute_enhanced_features(G, node, max_hops) for node in nodes}\n",
                "    return features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract and prepare features\n",
                "class_1_features = extract_features_for_classes(G_dir, merged_df, node_class=1, max_hops=3)\n",
                "class_2_features = extract_features_for_classes(G_dir, merged_df, node_class=2, max_hops=3)\n",
                "\n",
                "all_features = pd.DataFrame.from_dict(\n",
                "    {**class_1_features, **class_2_features}, orient=\"index\"\n",
                ")\n",
                "all_features[\"class\"] = [\"1\"] * len(class_1_features) + [\"2\"] * len(class_2_features)\n",
                "\n",
                "# Prepare the data\n",
                "X = all_features.drop(columns=[\"class\"])\n",
                "y = all_features[\"class\"].astype(int)\n",
                "\n",
                "# Split the data\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.3, random_state=42, stratify=y\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Random Forest Classifier"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hyperparameter grid\n",
                "param_dist = {\n",
                "    'classifier__n_estimators': [100, 250, 500, 750, 1000],\n",
                "    'classifier__max_depth': [5, 10, 15, 20, 25, None],\n",
                "    'classifier__min_samples_split': [2, 4, 6, 8],\n",
                "    'classifier__min_samples_leaf': [1, 2, 3, 4],\n",
                "    'classifier__class_weight': [None, 'balanced', 'balanced_subsample'],\n",
                "    'classifier__criterion': ['gini', 'entropy'],\n",
                "    'classifier__max_features': ['sqrt', 'log2', None]\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.feature_selection import SelectFromModel\n",
                "from sklearn.ensemble import ExtraTreesClassifier\n",
                "\n",
                "pipeline = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('feature_selector', SelectFromModel(ExtraTreesClassifier(n_estimators=50))),\n",
                "    ('sampler', EnhancedSampler(sampling_strategy='auto')),\n",
                "    ('classifier', RandomForestClassifier(random_state=42))\n",
                "])\n",
                "\n",
                "random_search = RandomizedSearchCV(\n",
                "    pipeline, \n",
                "    param_distributions=param_dist, \n",
                "    n_iter=100,  # Increased iterations\n",
                "    cv=5, \n",
                "    scoring='balanced_accuracy',\n",
                "    random_state=42,\n",
                "    n_jobs=-1\n",
                ")\n",
                "\n",
                "random_search.fit(X_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Best model evaluation\n",
                "best_model = random_search.best_estimator_\n",
                "y_pred = best_model.predict(X_test)\n",
                "\n",
                "# Print results\n",
                "print(\"Best Parameters:\", random_search.best_params_)\n",
                "print(\"\\nBest Cross-Validated Score:\", random_search.best_score_)\n",
                "print(\"\\nTest Set Balanced Accuracy:\", balanced_accuracy_score(y_test, y_pred))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Logistic Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from imblearn.pipeline import Pipeline\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "from imblearn.over_sampling import SMOTE\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import classification_report, confusion_matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "oversample = SMOTE(random_state=42)\n",
                "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
                "\n",
                "pipeline = Pipeline([('smote', oversample), \n",
                "                     ('logreg', lr)])\n",
                "\n",
                "param_grid = {\n",
                "    'smote__k_neighbors': [3, 5],\n",
                "    'logreg__C': [0.01, 0.1, 1, 10],\n",
                "    'logreg__class_weight': ['balanced', None]\n",
                "}\n",
                "\n",
                "grid_search = GridSearchCV(\n",
                "    pipeline, param_grid, cv=5, scoring='recall', n_jobs=-1\n",
                ")\n",
                "grid_search.fit(X_train, y_train)\n",
                "best_model = grid_search.best_estimator_\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict the test set labels\n",
                "\n",
                "\n",
                "\n",
                "y_pred = best_model.predict(X_test)\n",
                "\n",
                "# Generate a classification report\n",
                "# This shows precision, recall (which is the % of each category predicted correctly), and F1-score by class.\n",
                "print(\"Classification Report:\")\n",
                "print(classification_report(y_test, y_pred))\n",
                "\n",
                "# You can also print the confusion matrix\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "print(\"\\nConfusion Matrix (rows=actual, cols=predicted):\")\n",
                "print(cm)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Opaque Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv('elliptic_txs_features.csv')\n",
                "# Step 1: Move column names into the first row\n",
                "df = pd.concat([pd.DataFrame([df.columns], columns=df.columns), df], ignore_index=True)\n",
                "\n",
                "# Step 2: Rename columns sequentially from 1 to the number of columns\n",
                "df.columns = range(1, len(df.columns) + 1)\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "classes = pd.read_csv('elliptic_txs_classes.csv')\n",
                "classes.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df['class'] = classes['class']\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "labeld = df[df['class'] != 'unknown']\n",
                "labeld.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "labeld['class'].value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = labeld.drop([1, 'class'], axis=1)\n",
                "y = labeld['class']\n",
                "X.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X.dtypes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a list of columns from \"2\" to \"167\" (inclusive)\n",
                "cols_to_convert = [i for i in range(2, 168)]\n",
                "\n",
                "# Convert these columns to float\n",
                "X[cols_to_convert] = X[cols_to_convert].astype(float)\n",
                "X.dtypes\n",
                "\n",
                " "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# X = (X - X.mean())/X.std()\n",
                "# X.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "oversample = SMOTE(random_state=42)\n",
                "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
                "\n",
                "pipeline = Pipeline([('smote', oversample), \n",
                "                     ('logreg', lr)])\n",
                "\n",
                "param_grid = {\n",
                "    'smote__k_neighbors': [3, 5],\n",
                "    'logreg__C': [0.01, 0.1, 1, 10],\n",
                "    'logreg__class_weight': ['balanced', None]\n",
                "}\n",
                "\n",
                "grid_search = GridSearchCV(\n",
                "    pipeline, param_grid, cv=5, scoring='recall', n_jobs=-1\n",
                ")\n",
                "# Split the data\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.3, random_state=42, stratify=y\n",
                ")\n",
                "grid_search.fit(X_train, y_train)\n",
                "best_model = grid_search.best_estimator_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred = best_model.predict(X_test)\n",
                "\n",
                "# Generate a classification report\n",
                "# This shows precision, recall (which is the % of each category predicted correctly), and F1-score by class.\n",
                "print(\"Classification Report:\")\n",
                "print(classification_report(y_test, y_pred))\n",
                "\n",
                "# You can also print the confusion matrix\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "print(\"\\nConfusion Matrix (rows=actual, cols=predicted):\")\n",
                "print(cm)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
